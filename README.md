# Open Source LLMs List

> [!NOTE]
> Name that "*" is added is closed source, not open source.
> 
> Value that "?" is added might not be accurate, or might be myth (hallucination). 

| Name | by | URL | Demo | Size | Max Token | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC | AMC 2022-23 | MGSM |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| LLaVa-v1.5-13B | haotian-liu | https://github.com/haotian-liu/LLaVA | https://llava.hliu.cc/ | 13B | | 78.5 | 62.0 | 50.0 | 66.8 | 58.2 | 85.9 | 1510.7 | 64.3 | 58.3 | 58.6 | 65.4 | 31.1 |
| LLaVa-v1.5-7B | haotian-liu | https://github.com/haotian-liu/LLaVA |  | 7B | | 80.0 | 63.3 | 53.6 | 71.6 | 61.3 | 85.9 | 1531.3 | 67.7 | 63.6 | 61.6 | 72.5 | 36.1 |
| *Gemini Ultra | Google | https://blog.google/technology/ai/google-gemini-ai/ , https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf | No Demo is available | 10T? |
| *Gemini Pro | Google | https://blog.google/technology/ai/google-gemini-ai/ , https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf | Through Google API |  1.56T? |
| *Gemini Nano-1 | Google | https://blog.google/technology/ai/google-gemini-ai/ , https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf | Via Google Pixel 8 | 1.8B |
| *Gemini Nano-2 | Google | https://blog.google/technology/ai/google-gemini-ai/ , https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf | Via Google Pixel 8 | 3.25B |
| *GPT-4 | OpenAI | https://cdn.openai.com/papers/gpt-4.pdf , https://chat.openai.com | https://chat.openai.com | Unknown | 8192, 32768 |  |  |  |  |  |  |  |  |  |  |  | 86.4 5-shot |  |  |  | 67.0% 0-shot |  | 80.9 3-shot | 95.3 10-shot |  | 87.5 5-shot | 96.3 25-shot |
| *GPT-3.5 | OpenAI | https://chat.openai.com | https://chat.openai.com | Unknown? | Max Token | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) |
| *GPT-3.5-Turbo | OpenAI | https://chat.openai.com | https://chat.openai.com | Unknown? | Max Token | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) |
| *PaLM 2-L | Google | https://ai.google/discover/palm2/ |  |
| *Claude 2 | Anthropic | https://www.anthropic.com/index/claude-2 | https://claude.ai/ | Unknown | 100000 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC |
| *Claude 2.1 | Anthropic | https://www.anthropic.com/index/claude-2-1 | https://claude.ai/ | Unknown | 200000 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC |
| *Claude Instant | Anthropic |  | https://claude.ai/ | Unknown | 100000 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC |
| *GROK | X AI | https://grok.x.ai/ | Unavailable |
| LLaMa-2-70B | Meta | https://ai.meta.com/llama/ | https://sdk.vercel.ai/ | 70B | 4096 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | 68.9 | 56.8 | MATH | BIG-Bench-Hard | 29.9 | Natural2Code | DROP | 85.3 | WMT23 | 80.2 | AI2 Reasoning Challenge (ARC) | 85.0 | 33.0 | 54.2 (English only) | 85.0 | 60.2 | 49.3 |
| LLaMa-2-13B | Meta | https://ai.meta.com/llama/ | https://sdk.vercel.ai/ | 13B | 4096 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | 54.8 | 28.7 | MATH | BIG-Bench-Hard | 18.3 | Natural2Code | DROP | 80.7 | WMT23 | 72.8 | AI2 Reasoning Challenge (ARC) | 77.2 | 28.0 | 39.1 (English only) | 81.7 | 57.0 | 44.8 |
| LLaMa-2-7B | Meta | https://ai.meta.com/llama/ | https://sdk.vercel.ai/ | 7B | 4096 | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | 45.3 | 14.6 | MATH | BIG-Bench-Hard | 12.8 | Natural2Code | DROP | 77.2 | WMT23 | 69.2 | AI2 Reasoning Challenge (ARC) | 68.9 | 22.7 | 29.3 (English only) | 77.4 | 58.6 | 39.7 |
| *Gemini 1.5 Pro | Google | https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf | Not available | Unknown | 1M | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | 81.9% 5-shot | 91.7% 11-shot | 59.4% 7-shot | 84.0% 3-shot | 71.9% 0-shot | 77.7% 0-shot | 78.9% | 92.5% 10-shot | 75.20 1-shot | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC | 37.2% 4-shot | 88.73% 8-shot |
| Bloom | BigScience | https://huggingface.co/bigscience/bloom | https://huggingface.co/bigscience/bloom | 176B | Max Token | VQAv2 | GQA | VizWiz | SQA | T-VQA | POPE | MME | MM-Bench | SEED | LLaVA-Bench-Wild | MM-Vet | MMLU | GSM8K | MATH | BIG-Bench-Hard | HumanEval | Natural2Code | DROP | Hellaswag | WMT23 | WinoGrande | AI2 Reasoning Challenge (ARC) | Trivia QA | Natural Questions | AGI Eval | BoolQ | OpenBookQA | QuAC | AMC 2022-23 | MGSM |
